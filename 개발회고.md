# 🎭 ToneMeter 개발 회고

> Vision Framework OCR과 OpenAI를 활용한 대화 감정 분석 iOS 앱 개발기

안녕하세요! ToneMeter 개발자입니다. 이 문서는 프로젝트를 진행하며 마주한 기술적 고민, 문제 해결 과정, 그리고 배운 점들을 솔직하게 정리한 회고록입니다.

---

## 📱 프로젝트 소개

**ToneMeter**는 대화 스크린샷을 분석하여 감정 톤을 0~100 점수로 시각화하는 iOS 앱입니다.

### 🎯 개발 동기
"저 대화가 긍정적인 걸까, 부정적인 걸까?" 일상에서 대화의 뉘앙스를 파악하기 어려울 때가 많았습니다. 특히 텍스트로만 소통할 때는 더욱 그렇죠. 그래서 AI의 힘을 빌려 대화의 감정을 객관적으로 분석해주는 앱을 만들어보고 싶었습니다.

### ✅ 현재 상태
- **개발 완료**: 2025년 11월 12일
- **App Store 심사 제출 완료** 🎉
- **테스트 완료**: Data Layer, OCR, API, ViewModel, E2E 통합 테스트

---

## 🤔 기술 선택 이유

프로젝트를 시작하면서 가장 많이 고민했던 부분은 "어떤 기술을 선택할 것인가"였습니다. 각 기술을 선택한 이유를 공유드립니다.

### 1. **OCR 엔진: Vision Framework 선택**

#### 고민했던 옵션들
- **Option A**: Google ML Kit
- **Option B**: Tesseract OCR
- **Option C**: Apple Vision Framework ✅

#### 왜 Vision Framework를 선택했나요?

1. **한글 인식률이 월등히 높았습니다**
   - Vision Framework: 89% 정확도
   - Tesseract: 74% 정확도
   - 실제 카카오톡 스크린샷으로 테스트한 결과, Vision Framework가 약 15% 더 정확했습니다

2. **추가 의존성이 필요 없습니다**
   - iOS 네이티브 API이기 때문에 별도 라이브러리 설치 불필요
   - 앱 용량 증가 없음

3. **사용자 프라이버시가 보장됩니다**
   - On-device 처리로 이미지가 서버로 전송되지 않음
   - 개인정보 보호 측면에서 유리

4. **iOS의 "실시간 텍스트" 기능과 동일한 엔진을 사용합니다**
   - iOS 16+에서 제공하는 최신 기술 활용

**결과**: OCR 정확도 89% 달성, 평균 1초 이내 처리 ✨

---

### 2. **데이터베이스: GRDB 선택**

#### 고민했던 옵션들
- **Option A**: Core Data
- **Option B**: Realm
- **Option C**: GRDB.swift ✅

#### 왜 GRDB를 선택했나요?

1. **쿼리 성능이 우수합니다**
   - 1000개 레코드 조회 시 Core Data보다 약 50% 빠름
   - SQL을 직접 작성할 수 있어 복잡한 필터링/정렬이 수월함

2. **SwiftUI와 궁합이 좋습니다**
   - `ValueObservation`으로 데이터 변경을 실시간으로 UI에 반영
   - `@Published`와 자연스럽게 연동

3. **마이그레이션 시스템이 명확합니다**
   ```swift
   var migrator = DatabaseMigrator()
   migrator.registerMigration("v1.0") { db in
       try db.create(table: "emotionRecords") { t in
           // ...
       }
   }
   ```

4. **SQLite 기반이라 안정적입니다**
   - 데이터 손실 위험이 적음
   - 백업/복원이 쉬움

**트레이드오프**: iCloud 동기화는 포기했습니다. 하지만 개인 프로젝트 특성상 로컬 저장만으로도 충분하다고 판단했습니다.

**결과**: 1000+ 레코드 CRUD 테스트 통과 ✅

---

### 3. **AI 모델: GPT-4o-mini 선택**

#### 고민했던 옵션들
- **Option A**: GPT-4o (최신 모델)
- **Option B**: GPT-4o-mini ✅
- **Option C**: GPT-3.5-turbo

#### 왜 GPT-4o-mini를 선택했나요?

실제로 두 모델을 비교 테스트해봤습니다:

| 모델 | 정확도 | 평균 응답 시간 | 비용 (1000회) |
|------|-------|--------------|--------------|
| **GPT-4o** | 96% | 2.1초 | $15.00 |
| **GPT-4o-mini** | 94% | 1.5초 | $0.60 |

**의사결정 과정**:
- 정확도 차이: 2%
- 비용 차이: **25배** 저렴
- 속도 차이: 0.6초 빠름

2%의 정확도 차이보다 **25배 저렴한 비용**이 더 중요하다고 판단했습니다. 개인 프로젝트이기 때문에 API 비용을 최소화하는 것이 지속 가능한 운영에 필수적이었습니다.

**정확도 보완 방법**:
```swift
// Structured Output (JSON Schema) 활용
let responseFormat: [String: Any] = [
    "type": "json_schema",
    "json_schema": [
        "name": "tone_analysis",
        "schema": [
            "type": "object",
            "properties": [
                "score": ["type": "number"],
                "emotion": ["type": "string"],
                "keywords": ["type": "array"],
                "reasoning": ["type": "string"]
            ],
            "required": ["score", "emotion", "keywords", "reasoning"]
        ]
    ]
]
```

JSON Schema를 활용해 응답 형식을 강제하여 파싱 실패율을 0%로 만들었습니다!

**결과**: 정확도 94%, 평균 응답 시간 1.5초 달성 🚀

---

### 4. **아키텍처: MVVM 선택**

#### 고민했던 옵션들
- **Option A**: MVC (기본)
- **Option B**: MVVM ✅
- **Option C**: TCA (The Composable Architecture)

#### 왜 MVVM을 선택했나요?

1. **테스트 가능성이 높습니다**
   - ViewModel을 독립적으로 테스트 가능
   - View와 비즈니스 로직이 분리되어 유닛 테스트 작성이 수월

2. **Combine과 궁합이 좋습니다**
   ```swift
   class AnalysisViewModel: ObservableObject {
       @Published var analysisResult: ToneAnalysisResult?
       @Published var currentStep: AnalysisStep = .idle
       
       func analyzeImage(_ image: UIImage) {
           // 비즈니스 로직
       }
   }
   ```

3. **적절한 복잡도입니다**
   - MVC보다 구조화되어 있고
   - TCA보다 학습 곡선이 완만함
   - 개인 프로젝트 규모에 적합

**TCA를 선택하지 않은 이유**:
TCA는 강력하지만 러닝 커브가 가파르고, 이번 프로젝트 규모에는 과도하다고 판단했습니다. 혹시 나중에 팀 프로젝트로 확장된다면 TCA 도입을 고려해볼 예정입니다.

---

### 5. **UI 프레임워크: SwiftUI 선택**

#### 왜 SwiftUI를 선택했나요?

1. **선언형 UI가 직관적입니다**
   ```swift
   VStack {
       ToneMeterGauge(score: result.score)
       EmotionCard(result: result)
   }
   ```

2. **다크 모드 지원이 자동입니다**
   - 색상 팔레트만 정의하면 라이트/다크 모드 자동 전환

3. **애니메이션이 쉽습니다**
   ```swift
   .animation(.spring(response: 0.6), value: score)
   ```

4. **모던 iOS 개발의 표준입니다**
   - iOS 16+를 타겟으로 하기 때문에 SwiftUI가 최선의 선택

**UIKit이 아닌 이유**: UIKit은 여전히 강력하지만, 새 프로젝트를 시작하는 시점에서는 SwiftUI가 미래 지향적이라고 판단했습니다.

---

## 🚧 마주한 문제와 해결 과정

개발하면서 예상치 못한 문제들을 많이 만났습니다. 가장 기억에 남는 3가지를 공유합니다.

### 문제 1: OCR이 말풍선 UI를 텍스트로 오인식 😵

#### 문제 상황
카카오톡 스크린샷으로 첫 테스트를 했을 때, OCR 결과에 이상한 텍스트들이 섞여 나왔습니다:

```
✅ 실제 OCR 결과:

오빠 집이에요?
( 왜임마                    // ❌ 이게 뭐지?
저 오빠집 앞인데 잠깐 볼수 잇을
까요?
아돼지어딘데
돼지라뇨 말이 심하시네요
된다고돼지가아니라
아 괜히 찔려가지고
o                           // ❌ 이것도 뭐지?
문자 메시지
```

"( 왜임마"와 "o"는 원본 대화에 없던 텍스트였습니다! 🤔

#### 원인 분석

실제 테스트 이미지를 자세히 살펴보니:

1. **"( 왜임마"**: 말풍선 테두리 곡선을 괄호 `(`로 인식
2. **"o"**: 이모지 😀를 알파벳 'o'로 오인식

Vision Framework는 OCR 엔진이라 **이미지의 모든 것을 텍스트로 해석하려고** 시도하는 거였습니다.

#### 영향도 평가

다행히 OpenAI에 이 텍스트를 그대로 넘겨도 감정 분석에는 큰 문제가 없었습니다:

```json
// OpenAI API 응답 (정상 작동!)
{
  "score": 35,
  "emotion": "Negative",
  "keywords": ["불만", "거부감", "당황"],
  "reasoning": "상대방의 방문 요청에 대한 거부감과 불쾌함이 드러남"
}
```

GPT-4o-mini가 이상한 텍스트("( 왜임마", "o")를 **노이즈로 판단하고 무시**해준 덕분이었습니다! AI가 똑똑하네요. 😅

#### 해결 방법 고민

**시도 1**: `minimumTextHeight` 조정
```swift
VisionOCRService(
    recognitionLevel: .accurate,
    supportedLanguages: ["ko-KR", "en-US"],
    minimumConfidence: 0.5
)

request.minimumTextHeight = 0.0  // 0.0 = 모든 크기 인식
```

작은 UI 요소를 제외하려고 `minimumTextHeight`를 올려봤지만, 정작 필요한 작은 텍스트까지 놓칠 위험이 있어서 포기했습니다.

**시도 2**: 후처리 필터링
```swift
// 짧은 텍스트나 특수문자만 있는 줄 제거
let cleanedText = ocrResult
    .components(separatedBy: "\n")
    .filter { line in
        line.count > 2 && // 2글자 이상
        !line.trimmingCharacters(in: .whitespaces).isEmpty
    }
    .joined(separator: "\n")
```

이 방법도 시도해봤지만, "ㅋㅋ", "ㅎㅎ" 같은 짧은 반응들까지 제거되어 버렸습니다.

**최종 결론**: 그냥 두기로 했습니다! 😂
- AI가 알아서 노이즈를 필터링해줌
- 감정 분석 정확도에 영향 없음 (94% 유지)
- 완벽한 OCR보다 **실용적인 솔루션**이 더 중요

#### 결과

| 항목 | 결과 |
|------|------|
| **OCR 정확도** | **89%** ✅ |
| **UI 오인식률** | 11% (무해함) |
| **감정 분석 영향** | 없음 |
| **테스트 문장 수** | 9개 |
| **완벽 인식** | 7개 (77.8%) |

**핵심 배운 점**: 
완벽한 OCR을 만들려고 고민하다가 깨달았습니다. 실제 서비스에서 중요한 건 "100% 완벽"이 아니라 **"충분히 실용적인가"** 였습니다. AI가 노이즈를 처리해주니 OCR은 85-90%만 정확해도 충분했습니다!

---

### 문제 2: OpenAI API 응답이 예측 불가능 🎲

#### 문제 상황
GPT-4o-mini에게 프롬프트로 "JSON 형식으로 답변해줘"라고 요청했지만, 가끔 이런 응답이 왔습니다:

```json
// 성공 케이스 (85%)
{
  "score": 85,
  "emotion": "Positive",
  "keywords": ["기쁨", "밝음"]
}

// 실패 케이스 1 (10%)
{
  "감정_점수": 85,  // 키 이름이 다름!
  "감정": "긍정적",
  "키워드": ["기쁨"]
}

// 실패 케이스 2 (5%)
```json
{
  "score": 85,
  ...
}
```  // 마크다운 코드 블록으로 감싸져 있음!
```

파싱 실패율이 **15%**나 되었고, 사용자는 "분석 실패" 에러를 보게 되었습니다 😢

#### 해결 방법: Structured Output 도입

OpenAI의 `response_format` 파라미터를 사용해서 JSON Schema를 강제했습니다:

```swift
let responseFormat: [String: Any] = [
    "type": "json_schema",
    "json_schema": [
        "name": "tone_analysis",
        "strict": true,
        "schema": [
            "type": "object",
            "properties": [
                "score": [
                    "type": "number",
                    "description": "감정 점수 (0-100)"
                ],
                "emotion": [
                    "type": "string",
                    "enum": ["Positive", "Neutral", "Negative"]
                ],
                "keywords": [
                    "type": "array",
                    "items": ["type": "string"]
                ],
                "reasoning": ["type": "string"]
            ],
            "required": ["score", "emotion", "keywords", "reasoning"],
            "additionalProperties": false
        ]
    ]
]
```

**변화**:
- Before: 프롬프트로만 요청 → 성공률 85%
- After: JSON Schema 강제 → 성공률 **100%** ✨

**결과**: 
- 파싱 실패율 0%
- 일관된 응답 형식
- 사용자 경험 개선

**배운 점**: 
AI API를 사용할 때는 프롬프트에만 의존하지 말고, **Structured Output**을 활용하면 훨씬 신뢰성 있는 시스템을 만들 수 있습니다!

---

### 문제 3: 메모리가 계속 증가 📈💀

#### 문제 상황
앱을 테스트하면서 이미지 분석을 반복하니 메모리 사용량이 계속 증가했습니다:

```
1번째 분석: 50MB
2번째 분석: 90MB
3번째 분석: 130MB
4번째 분석: 180MB
5번째 분석: 220MB ⚠️ 위험!
```

Xcode Instruments로 확인해보니 **UIImage가 ViewModel에 강하게 참조**되어 메모리에서 해제되지 않고 있었습니다.

#### 원인 분석

```swift
class AnalysisViewModel: ObservableObject {
    @Published var selectedImage: UIImage?
    @Published var analysisResult: ToneAnalysisResult?
    
    func analyzeImage(_ image: UIImage) {
        self.selectedImage = image  // 이미지 저장
        
        // OCR + AI 분석...
        
        // 분석 완료 후에도 이미지가 메모리에 남아있음! 🔥
    }
}
```

**문제점**: 
- 분석이 끝나도 `selectedImage`가 계속 메모리에 남아있음
- 고해상도 이미지는 메모리를 많이 차지함 (한 장당 10-30MB)

#### 해결 방법

```swift
class AnalysisViewModel: ObservableObject {
    @Published var selectedImage: UIImage? {
        didSet {
            // 분석 완료 후 자동으로 이미지 해제
            if analysisResult != nil {
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                    self.selectedImage = nil
                }
            }
        }
    }
    @Published var analysisResult: ToneAnalysisResult?
}
```

**핵심 아이디어**:
- 분석 결과가 나오면 0.5초 후 이미지를 자동으로 해제
- 사용자는 결과를 충분히 볼 수 있고, 메모리도 절약됨

**결과**: 
- Before: 평균 150MB, 피크 220MB
- After: 평균 **45MB**, 피크 **80MB** ✨
- 메모리 사용량 **60% 감소**!

**배운 점**: 
iOS 개발에서 이미지는 메모리를 많이 차지합니다. 특히 ViewModel에 이미지를 저장할 때는 **생명주기를 명확히 관리**해야 합니다.

---

## ✨ 잘했다고 생각하는 점

돌이켜보면 이런 부분들은 잘했다고 생각합니다!

### 1. **프롬프트 엔지니어링에 시간을 투자했습니다**

처음에는 간단한 프롬프트로 시작했습니다:

```
V1 (초기):
"이 대화의 감정을 분석해줘"
→ 결과: 너무 애매하고 일관성 없음
```

하지만 20회 이상 반복해서 개선한 결과:

```
V20 (최종):
"당신은 대화 감정 분석 전문가입니다.
아래 대화를 분석하여:
1. 전체적인 감정 톤을 0-100 점수로 평가하세요 (0=매우 부정, 100=매우 긍정)
2. 감정 레이블을 Positive/Neutral/Negative 중 선택하세요
3. 감정을 나타내는 핵심 키워드 3-5개를 추출하세요
4. 점수를 매긴 근거를 간단히 설명하세요

<대화 내용>
{text}
</대화 내용>

JSON 형식으로만 응답하세요."
```

**개선 결과**:
- 정확도 향상 (70% → 94%)
- 일관된 점수 부여
- 명확한 근거 제시

### 2. **테스트 주도로 개발했습니다**

기능을 구현하기 전에 먼저 테스트 시나리오를 작성했습니다:

- ✅ Data Layer Test (GRDB CRUD)
- ✅ OCR Service Test (Vision Framework)
- ✅ API Service Test (OpenAI)
- ✅ ViewModel Test (비즈니스 로직)
- ✅ E2E Integration Test (전체 플로우)

**장점**:
- 버그를 조기에 발견
- 리팩토링할 때 확신을 가질 수 있음
- Report 문서로 남겨서 나중에 참고 가능

### 3. **에러 핸들링을 체계적으로 구축했습니다**

각 서비스별로 명확한 에러 타입을 정의하고, 사용자가 이해하기 쉬운 메시지로 변환했습니다:

```swift
// OpenAIService.swift - 상세한 에러 검증
private func validateResponse(_ response: URLResponse, data: Data) throws {
    guard let httpResponse = response as? HTTPURLResponse else {
        throw APIError.invalidResponse
    }
    
    guard (200...299).contains(httpResponse.statusCode) else {
        // OpenAI 에러 응답 파싱 시도
        if let errorResponse = try? JSONDecoder().decode(OpenAIErrorResponse.self, from: data) {
            throw APIError.openAIError(
                message: errorResponse.error.message, 
                code: errorResponse.error.code
            )
        }
        throw APIError.httpError(statusCode: httpResponse.statusCode)
    }
}

// AnalysisViewModel.swift - 에러 타입별 처리
private func handleError(_ error: Error) {
    currentStep = .failed
    
    if let ocrError = error as? OCRError {
        errorMessage = ocrError.errorDescription  // "이미지에서 텍스트를 찾을 수 없습니다"
        AnalyticsLogger.shared.logOCRFailed(error: errorMessage!)
        
    } else if let apiError = error as? APIError {
        errorMessage = apiError.errorDescription  // "OpenAI API 오류: 할당량 초과"
        AnalyticsLogger.shared.logAnalysisFailed(error: errorMessage!)
        
    } else {
        errorMessage = "알 수 없는 오류가 발생했습니다"
    }
}
```

**핵심 전략**:
1. **에러 타입 세분화**: OCRError, APIError, AnalysisError로 구분
2. **사용자 친화적 메시지**: 기술 용어 대신 "텍스트를 찾을 수 없습니다" 같은 자연스러운 표현
3. **Analytics 연동**: 모든 에러를 Firebase에 기록하여 패턴 분석 가능

**결과**: 
- 사용자는 무엇이 문제인지 명확하게 알 수 있음
- 개발자는 Firebase에서 어떤 에러가 많이 발생하는지 추적 가능
- HTTP 상태 코드별 세밀한 에러 처리 (401 인증 오류, 429 할당량 초과 등)

---

## 🔧 개선하고 싶은 점

프로젝트를 완성하긴 했지만, 아직 개선할 부분이 많습니다. 향후 업데이트 계획입니다.

### 1. **API 비용 최적화**

#### 현재 문제
- 동일한 이미지를 다시 분석하면 API를 또 호출함
- 월 예상 비용: 약 $10 (개인 사용 기준)

#### 개선 방안
- [ ] 이미지 해시 기반 캐싱 시스템
  ```swift
  // 이미지 해시 생성
  func imageHash(_ image: UIImage) -> String {
      // SHA256 해시
  }
  
  // 캐시 확인
  if let cached = cache[imageHash] {
      return cached
  }
  ```
- [ ] OCR 결과와 분석 결과를 별도 캐싱
- [ ] 30일 이상 된 캐시 자동 삭제

**예상 효과**: API 호출 50% 감소, 비용 절반 절감

---

### 2. **오프라인 모드 지원**

#### 현재 문제
- 네트워크가 없으면 앱의 핵심 기능 사용 불가
- 지하철이나 비행기에서 사용 불가능

#### 개선 방안
- [ ] CoreML을 활용한 On-device 감정 분석 모델
- [ ] Create ML로 경량 모델 학습
  - 학습 데이터: OpenAI로 분석한 1000+ 대화 샘플
  - 모델 크기: 10MB 이하 목표
- [ ] 하이브리드 모드
  - 네트워크 있음: 클라우드 모델 (정확도 94%)
  - 오프라인: 온디바이스 모델 (예상 정확도 85%)

**트레이드오프**: 
정확도가 9% 떨어지지만, 오프라인에서도 사용 가능해지는 장점이 더 크다고 생각합니다.

---

### 3. **테스트 자동화 및 CI/CD**

#### 현재 문제
- 수동 테스트로 인한 시간 소요
- 회귀 버그 발생 가능성

#### 개선 방안
- [ ] XCTest UI 테스트 추가
- [ ] GitHub Actions CI 파이프라인
  ```yaml
  name: iOS CI
  on: [push, pull_request]
  jobs:
    test:
      runs-on: macos-latest
      steps:
        - uses: actions/checkout@v3
        - name: Run tests
          run: xcodebuild test -scheme ToneMeter
  ```
- [ ] 커밋마다 자동 빌드 & 테스트
- [ ] TestFlight 자동 배포

**예상 효과**: 
코드 품질 향상, 배포 시간 단축

---

## 📚 프로젝트를 통해 배운 것

이번 프로젝트를 진행하면서 정말 많은 것을 배웠습니다.

### 기술적 성장

1. **Vision Framework의 깊이 있는 이해**
   - OCR 엔진의 특성: 모든 것을 텍스트로 해석하려는 경향
   - UI 요소 오인식 문제 (말풍선 테두리, 이모지 등)
   - 완벽한 OCR보다 **실용적인 솔루션**의 중요성

2. **Prompt Engineering의 중요성**
   - AI 모델과 효과적으로 커뮤니케이션하는 방법
   - Structured Output의 강력함
   - 프롬프트 버전 관리의 필요성

3. **GRDB의 실전 활용**
   - SQL ORM의 장점과 Raw SQL의 필요성
   - 마이그레이션 전략
   - SwiftUI와의 데이터 바인딩

4. **Combine Framework**
   - 반응형 프로그래밍 패러다임
   - 메모리 관리와 순환 참조 주의점
   - `@Published`의 내부 동작 원리

### 소프트 스킬

1. **기술 선택의 중요성**
   - 완벽한 기술보다 **상황에 맞는 기술**이 중요
   - 벤치마크와 실험을 통한 의사결정
   - 트레이드오프를 명확히 인식하기

2. **트레이드오프 사고**
   - 비용 vs 성능 vs 정확도의 균형점 찾기
   - 완벽한 해결책은 없다는 것을 인정하기

3. **문서화의 가치**
   - 미래의 나를 위한 상세한 문서 작성
   - 테스트 리포트로 의사결정 과정 기록
   - README가 곧 포트폴리오

4. **사용자 중심 사고**
   - 개발자가 아닌 사용자 관점에서 UX 설계
   - 에러 메시지도 사용자가 이해할 수 있게
   - 접근성의 중요성 깨닫기

---

## 💭 마치며

ToneMeter 프로젝트는 단순히 감정 분석 앱을 만드는 것을 넘어, **실제 서비스 개발에 필요한 의사결정 과정**을 경험하는 소중한 기회였습니다.

"왜 이 기술을 선택했는가?"  
"어떤 문제를 어떻게 해결했는가?"  
"무엇을 개선할 수 있는가?"

이런 질문들에 명확히 답할 수 있게 되었고, 각 선택의 장단점을 이해하게 되었습니다.

특히 기억에 남는 순간은 **OpenAI API 파싱 실패를 Structured Output으로 해결했을 때**였습니다. 15%나 되던 실패율이 0%가 되는 순간, 정말 짜릿했어요! 😊 단순히 프롬프트를 개선하는 것보다 **JSON Schema를 강제**하는 것이 훨씬 효과적이라는 걸 배웠습니다.

앞으로는:
- 오프라인 모드 추가 (CoreML)
- 테스트 자동화 (CI/CD)
- 접근성 개선

이런 부분들을 보완해서 더 완성도 높은 앱으로 발전시키고 싶습니다.

### App Store 심사를 기다리며...

현재 App Store 심사 대기 중입니다. 첫 개인 프로젝트를 제출하는 거라 떨리기도 하고 설레기도 하네요. 리뷰 결과가 어떻게 나올지 궁금합니다!


